\chapter{Conclusions and Future Work}
In this chapter we will summarize the conclusions we reached and give directions for future work.

\section{Conclusions}
In this project we created a file fragment classifier for document-type fragments. We used a large data set of about 20Gb size which contained files from 10 different file formats. We made use of the a variation of Byte Frequency Analysis algorithm to classify document-type fragments in a broader class.After this point we created a custom algorithm in order to be able to distinguish between the files that were classified initially as text from the BFA. Our results show that the BFA variation that we used is quite effective in distinguishing between document-type fragments from other file formats. 

Additionally, we introduced two new classification metrics. The Individual Null Byte Frequency(INBF) and the Plain Text Concentration(PTC). The INBF metric can be used to enhance xls fragments classification due to the fact that fragments of that file type have a significantly higher amount of null bytes compare to the fragments of the other 9 file types that we used. Moreover, the PTC metric is a very interesting finding. Although out of hindsight it looks obvious that files from specific file types will have characteristic plain text concentration, we couldn't find any reference in the existing bibliography. We strongly believe that this extremely light-weight metric can be combined with current techniques and improve their classification accuracy. 

In order to evaluate the performance of our classifiers we compared it with 4 different classification algorithms. As it seems our classifier is significantly more accurate with the decisions that it takes achieving an overall accuracy of 77\%. It did extremely well in classifying fragments of the text and xls file formats along with fragments of the non-document types, achieving an $F-score$ of 91\%, 75\% and 86\% respectively. 

Furthermore, we observed that most of the studies upon the file type validation field do research on techniques that are quite expensive. In addition, we experimented with the Longest Common Subsequence technique which was firstly introduced to the field by\cite{Calhoun}. We concluded that although this technique is extremely accurate for two-group classification, it cannot be used for broad fragment classification as Calhoun suggests, due to its high runtime complexity.

Lastly, the availability of a multitude of different file types in combination with the newly introduced file formats along with the accuracy and speed requirements of the forensic cases, render file fragment classification a wicked problem. Although we achieved quite good performance in both accuracy and runtime performance compared to already existing techniques, it still takes days for our unoptimized classifier to analyse 10GB of data. However, fragment classification has received little attention the past couple of years and we are confident that there is plenty of room for improvements. 

