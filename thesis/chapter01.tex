%This is chapter 1
%%=========================================
\chapter{Introduction}
File recovery from digital data storage devices has been a hot topic among
the Digital Forensics field. Traditional data storage devices make use of
file systems, in order to manage contained data, their available space and to
maintain location of files. When the storage device and its file system are
intact, it is quite simple to recover data from them. This is mainly because
file systems make use of meta-data in order to track information for their
files. Meta-data can contain information such as creation date, data struc-
ture (e.g directory or regular file), file type, file owner, size, last modified
date etc. In a real life forensic case it is highly unlikely that file meta-data will be
present, or they might be corrupted or deleted. It became clear for the digi-
tal forensic community that an alternative, more realistic approach must be
used.

%%=========================================
\section{File Carving}
File carving is a forensics technique that recovers files based on their content,
without relying on their meta-data. File carving process involves two steps.
File validation and file reconstruction.[1]. During the recovery procedure,
we must first validate the type of the file and then apply the appropriate
reconstruction technique. In this thesis, only the validation techniques are
of our interest.
By examining the byte-content and/or the structure of
a file [22], file validation techniques are used to classify its type. Several file
types contain common structures like headers, footers (named Magic Number
Matching) [7][3], fields that specify file attributes like color or size etc.(Data
Dependency Resolving [3]), that can be used to identify the type of the file.
Additionally, another approach is to apply statistical analysis techniques and
algorithms, which use the complete byte set of a file, creating a fileprint for
every file type. Some examples are the n-Gram Analysis [9], the Byte fre-
quency analysis (BFA) algorithm and the Byte frequency cross-correlation
(BFC)[4].
The aforementioned techniques have some profound weaknesses. The Magic
Number Matching and the Data Dependency Resolving approaches make
general type classification infeasible. This is due to the fact that not every
file-type contain such structures. Furthermore, n-Gram Analysis and both
BFA and BFC were designed to be applied in a complete file or a pre-defined
part of it, which retains all of its content. Hence, they depend on files internal structure and characteristics.
%%=========================================
\section{Problem Formulation}
So why this is a problem? The answer lies in file systems behaviour and
file fragmentation. When we delete a file from a media storage, the data are
not actually removed. The sectors in which the file was stored still contain
the same data, although the file system marks them as unallocated [2]. Which
means that the next time a new file is created, the file system is free to use
these sectors, which are marked as unallocated, to store the new file. But
if the new file is bigger than the old one, and the file system tries to store it
starting from the same sector entry as the deleted one, it wont have enough
space to store it. So the file system will allocate all the sectors of the previous deleted file, while the remaining data which do not fit, will be stored to
other unallocated sectors. This results to file fragmentation.\\

In a forensic file recovery case, it is more probable that the files that must be recovered are
fragmented. Validation techniques which use the complete file content
are highly unlikely to provide aid to forensic examiners. Hence, an alternative
approach to file type classification must be taken.
File fragment classification is a technique that uses only a small fragment
of a file, in order to determine its type. Ergo, file fragmentation is not a
problem any more as this approach is independent from files overall structure. Although in theory, file fragment classification looks like an ideal approach, in practice current solutions that use this approach could not yield
good results[][]. One reason that file fragment classification is difficult, is
due to the complex container files. Complex container files like TAR, ZIP,
RAR, PDF etc. contain other primitive file types, making general fragment
classification difficult. Moreover, a fragment might contain more data which
are strongly related to the files content than the files structure.
%%=========================================

%%=========================================
\section{Objectives}
The main objectives in this project are:
\begin{enumerate}
\item Test the hypothesis that by analysing only a special ASCII byte-set of file fragments, which corresponds to the printable ASCII characters plus the tab, newline and carriage return  character, accuracy of classification algorithms can be enhanced for document-type fragments.
\item Create a more accurate algorithm for identifying document-type fragments than the available ones. In particular text, xls, doc and pdf files are our main focus and we try to improve their classification accuracy.

\end{enumerate}

%%=========================================
\section{Algorithms Requirements}
The design requirements for our classification algorithm are as follows:
\begin{enumerate}
\item Speed - Relatively fast compared to current techniques
\item Accuracy - Algorithm should be as accurate as possible by minimizing false positives in classification of file fragments
\item Operate in 512 bytes, same as the sector size of a hard drive

\end{enumerate}

\pagebreak
%%=========================================
\section{Methodology}
Most of the current file and fragment classification techniques use the whole byte content of a file/fragment for both the training and classification procedures. Since we intend to create an algorithm which would be able to yield better accuracy results for fragments that originate from a document file type, we want to test the hypothesis that by using only the printable ASCII characters (32 $\geq$  b $\leq$ 126) plus the tab, newline and carriage return of a fragment we could achieve better results regarding text fragment classification. The aforementioned special characters are a behavioural trait of a document so we expect that their occurrence in conjunction with all the other printable characters will be more frequent in a document file.\\

In order to test our hypothesis we have to use one of the current classification algorithms in order to compare their accuracy results. Additionally, since our main goal is to design a classification algorithm  which will satisfy the already mentioned requirements, we should carefully choose a currently available algorithm that has the potential to be easily modified, without adding additional complexity, and to create a custom more effective version of it.

Our algorithm of choice is the Byte Frequency Analysis algorithm. More about the reasons of this choice can be found in the chapter 3.\\


 It has been observed that BFA, although extremely inaccurate, classifies a big amount of fragments that belong to a document file as text. We will make use of the BFA which will take under account only our special subset of ASCII characters among with 3 more variations of it and try to enhance its accuracy on classifying document-file fragments as text.  We used 4 different variations of the BFA mainly for 2 reason.  The first one is to compare the results with the results of the BFA that Shahi used for file fragment classification and find out if our hypothesis is correct. The second one is to choose the variation of it that yields the best results regarding text fragment classification. Literally this is going to be the first step of our algorithm so accuracy of our BFA variation will affect the accuracy of our final algorithm. Next we will isolate all fragments classified as text and analyse them in order to find patterns which will eventually result in special metrics that could help us to design our algorithm.
 
 % Moreover, we are going to use
 % the same corpus as Shahi did in [XX] , a subset of the training set that he used and the exact same testing set, in order to test    e          ffectively the accuracy of our algorithm. %


