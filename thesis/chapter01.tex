%This is chapter 1
%%=========================================
\chapter{Introduction}
File recovery from digital data storage devices has been a hot topic among
the Digital Forensics field. Traditional data storage devices make use of
file systems, in order to manage contained data, their available space and to
maintain location of files. When the storage device and its file system are
intact, it is quite simple to recover data from them. This is mainly because
file systems make use of metadata in order to track the metadata for their
files. Meta-data can contain information such as creation date, data structure (e.g directory or regular file), file type, file owner, size, last modified
date and more. In practice, most data can be recovered using the regular file system, but often investigators are specifically interested in the data that appears to be missing. In a real life forensic case, it is likely that a part of file systems metadata might be corrupted or deleted. 

%%=========================================
\section{File Carving}
File carving is a forensics technique that recovers files based on their content,
without relying on their meta-data. File carving process involves two steps.
File format validation and file reconstruction ~\cite{Aronson}. During the recovery procedure,
forensics investigators must first validate the type of the file and then apply the appropriate
reconstruction technique. At this point we should note that in this thesis, only the file format validation techniques are
of our interest.
By examining the byte-content and/or the structure of
an unclassified file, file format validation techniques are used to classify its type. The Magic Number Matching technique~\cite{Scalpel} looks for magic numbers, specific byte sequences that signal the beginning and/or the end of a file(headers,footers) and try to classify them to a file type according to that information. For example jpeg files begin with the hexadecimal sequence "FFD8" and end with "FFD9" \cite{Pal}.
Similarly, the Data Dependency Resolving technique is used to identify fields that specify file attributes like color or size \cite{Aronson}.
 
Furthermore, other file carving techniques use statistical learning algorithms, which process the complete byte set of a file, creating a representative fingerprint for
every file type. A classifier compares these fingerprints with an unidentified byte sequence and produce an accuracy level for each fingerprint. Then, it classifies the unidentified byte sequence with the file type of the fingerprint that yielded the highest accuracy level. Some common statistical learning techniques are the n-Gram Analysis \cite{ngram} and the Byte Frequency Analysis(BFA)  and the Byte frequency cross-correlation(BFC) algorithms \cite{McDaniel}.

%%=========================================
\section{Problem Formulation}
The aforementioned techniques have some profound weaknesses. The Magic
Number Matching and the Data Dependency Resolving approaches make
general type classification infeasible. This is due to the fact that not every
file-type contain such characteristic structures ~\cite{MacDaniel}. Furthermore, n-Gram Analysis and both
BFA and BFC were designed to be applied in a complete file or a pre-defined
part of it, which retains all of its content. Hence, they depend on files overall internal structure and characteristics.

So why this is a problem? The answer lies in file systems behaviour and
file fragmentation. When we delete a file from a data storage device, the data are
not actually removed. The sectors in which the file was stored still contain
the same data, although the file system marks them as unallocated \cite{Pal}. That
means the next time a new file is created, the file system is free to use
these sectors, which are marked as unallocated, to store a new one. But
if the new file is bigger than the old one, and the file system tries to store it
starting from the same sector entry as the deleted one, it wont have enough
space to store it. So the file system will allocate-overwrite all the sectors of the previous deleted file, while the remaining data which do not fit, will be stored in
other unallocated sectors. This results to file fragmentation.

Although fragmentation in current file systems is small \cite{Garfinkel}, the classification of the missing fragmented parts of a file are essential for its recovery. In that case, validation techniques which use the complete file content
are unable to provide aid to forensic examiners. %Hence, an alternative
%approach to file type classification must be taken.

\section{File Fragment Classification}
File fragment classification is a technique that uses only a small fragment
of a file, in order to determine its type. This approach is independent from files overall structure. Although in theory, file fragment classification looks like an ideal approach, in practice it proved to be difficult to create a technique of high precision ~\cite{Ashim}. It is noteworthy that in the last Digital Forensic Research Workshop(DFRWS 2012) challenge, the winning classification tool achieved an overall classification accuracy of 36\% ~\cite{Sceadan}, in a corpus of 38 different file types.

%%=========================================

%%=========================================
\section{Objectives}
The main objectives in this project are:
\begin{enumerate}
\item Create a classification algorithm ,for identifying document-type fragments, of higher precision that the existing similar algorithms. In particular, we focus on the classification of text, xls, doc and pdf file fragments and try to improve their classification precision.
\item Test the hypothesis that by analysing only a special ASCII byte-set of file fragments which corresponds to the printable ASCII characters, accuracy of classification algorithms can be enhanced for document-type fragments. This ASCII subset is comprised of byte values of the range ( $32\geq$ b $\leq 126$) among with the tab(9), new line(10) and carriage return(13) bytes. From this point, we will refer to this special ASCII subset as "plain text".




\end{enumerate}

%%=========================================
\section{Algorithms Requirements}
The design requirements for our classification algorithm are as follows:
\begin{enumerate}
\item Speed - Comparable in runtime performance to the current light-weight algorithms such as the N-Gram Analysis \cite{ngram}~\cite{Ashim} and the BFA algorithm ~\cite{MacDaniel}~\cite{Ashim}.
\item Accuracy - Improve upon the overall accuracy of the algorithms in the same runtime performance class.
\item Operate in common fragment sizes, preferably of 512-bytes size, the smallest relevant size which is also equivalent with a hard drives sector size.
\end{enumerate}

%%=========================================
\section{Methodology}
Most of the current file and fragment classification techniques use the whole byte content of a file/fragment for both the training and classification procedures. Since we intend to create an algorithm which would be able to yield better accuracy results for fragments that originate from a document file type, we want to test the hypothesis that by using only the plain text ASCII subset of a fragment, we could achieve better results regarding text fragment classification. The plain text characters are a behavioural trait of a document so we expect that their occurrence will be more frequent in a document file.

To test our hypothesis we have to use one of the current classification algorithms in order to compare their accuracy results. Additionally, since our main goal is to design a classification algorithm  which will satisfy the already mentioned requirements, we should carefully choose a currently available algorithm that has the potential to be easily modified, without adding significant complexity, and to create a custom more effective version of it. Our algorithm of choice is the Byte Frequency Analysis ~\cite{MacDaniel}. More about the reasons of this choice can be found in chapter 3.

Our design procedure is comprised of two main phases. In phase 1 we intent to use a BFA that uses only the plain text byte set for a fast scan of the corpus, in order to isolate a big amount of document-type fragments. In phase 2 we analyse the complete ASCII byte set of BFA output and try to classify of what types these document-type fragments are.


During phase 1, we use 4 variations of BFA that analyse only the plain text of the input fragments and also test our hypothesis. We compare the results of these variations with each other and we choose the one that yields the best results regarding text fragment classification. By text fragment classification we mean the classification of document-type fragments like pdf, text, doc and xls as text. After that we compare the best BFA variation  with the default BFA, which takes under account the whole ASCII byte set, that Shahi \cite{Ashim} used in a corpus comprised of the same file types as the one we use. Finally, we choose the variations that yields the best results. This BFA variation will be the first part of our final algorithm. Thereafter, we isolate all fragments that were classified as text, resulting in a new corpus and proceed in phase 2 of our design procedure.

During phase 2, we analyse the whole byte content of the fragments that were classified as text, trying to find patterns that could aid our algorithms design. Initially we used simple statistical metrics such as the mean, mode, median and standard deviation trying to find characteristic patterns in specific file types. This resulted to focus on some specific byte sequences, where in conjunction with the histogram analysis we did, resulted in the discovery of two new metrics. The Individual Null Byte Frequency and the Plain Text Concentration Categories. We combine these two metrics along with the Shannon entropy metric ~\cite{Shannon} and the accuracy levels that BFA produced in phase 1, to create a new custom algorithm.

%Finally, we make use of a new corpus in order to test our algorithms accuracy. This corpus intentionally has the same file types and the same size as the testing set that Shahi used in \cite{Ashim}. This way we can compare our algorithms performance with the algorithms that Shahi used.



 


