\chapter{Classification  Metrics}
\section{BFA Variation 1 Output}
After the run of variation 1 BFA, we isolated all fragments which were classified as text. Initially, we expected that BFA falsely classifies fragments from non-text files as text, due to their high plain text concentration. We conducted a plain text concentration analysis on the BFAs output and it seems that BFA classified as text fragments with diverse plain text concentration. This analysis can be found in Table \ref{table:bfas_output}.
\input{bfas_output_table}

 Although the 85\% of BFAs output  originates from document-type files, our algorithms considers all these fragments to be of xls, pdf, doc and text type. By doing this, we expect that the amount of fragments that were falsely classified as text without belonging to a document-type file, will be evenly distributed among the false positive classification results for xls, pdf, doc and text fragments. Our algorithms goal is to be able to correctly identify and distinguish between xls, pdf, doc and text fragments. For that purpose we conducted statistical analysis in the BFAs output trying to find patterns that will help as increase our algorithms accuracy. We introduce two new metrics, the individual null byte frequency and the plain text ratio category.  The individual null byte frequency in conjunction with shannon entropy[] can be used to effectively distinguish  between pdf from xls and doc fragments. Additionally, the plain text ratio category metric can be used to prevent our algorithm to falsely classify a fragment that belongs to a certain plain text ratio category.


\section{Plain Text Concentration Categories}
As we already mentioned, file fragments of certain types are expected to have certain plain text concentration. We use 4 concentration categories of equal size. 0-25\%, 25-50\%, 50-75\% and 75-100\%. Our metric assumes that fragments are of 512-bytes size. As we saw in Table **4.2**, 75\% or more of text fragments is plain text and the majority of xls fragments(97\%) are 0-50\% plain text. Moreover more than 90\% of the total mp4, zip, ppt, jpg, png and ogg fragments are 25-50\% plain text. Additionally, we run an extra analysis specifically for the text fragments and we found that 98\% of them are fully comprised of plain text. We are positive that this light weight metric can be combined with current techniques and increase their accuracy. For example if a fragment is classified as text and it contains at least one non-plain text byte, then probably it's not a text fragment. So a classification algorithm could make this simple check and substitute its first classification "guess" with the one that had the second highest accuracy level. Similarly, if a fragment is more than 75\% plain text then probably it's not a mp4, zip or ogg fragment etc.

\section{Individual Null Byte Frequency}
We applied several statistical metrics such as median, mean, mode, standard deviation, minimum and maximum frequency byte values in the BFAs output fragments. However, we couldn't find something extra that could significantly aid our algorithms design. Thereafter, we manually inspected several fragments from all the file types, and we noticed that the amount of null bytes in xls fragments was significantly high. However, although slightly less, the frequencies of null bytes was also similar for doc and pdf fragments. We noticed that there were many long sequences of null bytes in most of the pdf and doc fragments but in the xls fragments these sequences were significantly fewer. Additionally, the majority of the total null bytes in xls fragments were individual. Therefore, we analysed the distribution of individual null bytes for all the document-type fragments. As you can see in figures 1,2,3,4 the number of individual null bytes in xls fragments is obviously higher than the one of the other file types. For text fragments, the amount of individual null bytes is 0 and for pdf and doc fragments the frequency mainly ranges from 0 to 25. Since the majority of text fragments are fully comprised of plain text, it's natural that they do not contain null values.




\section{Shannon Entropy}
There is a widespread use of the Shannon entropy[Shannon] metric in file carving techniques. Entropy measures how much information a sequence of symbols contains[Shannon,Calhoun]. Entropy is defined as:
 \begin{displaymath}
 H({X_i}..{X_n})=-\sum_{i=0}^{n}{p({x_i})}\log_2{p({x_i})}
\end{displaymath} 

In our case, $X={X_i}..{X_n}$  is the byte-content of a fragment, where $n=511$  and $p({x_i})$ is the frequency of ${x_i}$ in ${X}$. To calculate $p({x_i})$, we simply divide the number of occurrences of ${x_i}$ in a fragment with the fragments size. It is known that usually compressed files have high entropy in contrast with text files that have low entropy[Calhoun,Jeroen-Thesis]. Since pdf is a compressed file format, we expected that pdf fragments will have significantly higher entropy than doc, xls and text fragments. In figures 1,2,3,4 we can see the entropy distribution among these file fragments. Most of the pdf fragments have an entropy value of 6 or more, in contrast with the other file-type  where the majority of their fragments has an entropy of value 6 or less.

