\chapter{Classification  Metrics}
\section{BFA Variation 1 Output}
After the run of variation 1 BFA, we isolated all fragments which were classified as text. We believed that BFA falsely classifies fragments from non-text files as text, due to their high plain text concentration. We conducted a plain text concentration analysis on the BFAs output and it seems that BFA classified as text fragments with both very low and high plain text concentration. This analysis can be found in Table X.

 Since the 85\% of them originates from xls, pdf, doc and text files, we consider the remaining 15\%, which corresponds to fragments from the other 6 file types, that do not exist. So by doing this, we expect that the amount of fragments that were falsely classified as text without belonging to a document-type file, will be evenly distributed among the false positive classification results for xls, pdf, doc and text fragments. Our algorithms goal is to be able to correctly identify and distinguish between xls, pdf, doc and text fragments. For that purpose we conducted statistical analysis in the BFAs output trying to find patterns that will help as increase our algorithms accuracy. We introduce two new metrics, the individual null byte frequency and the plain text ratio category.  The individual null byte frequency in conjunction with shannon entropy[] can be used to effectively distinguish  between pdf from xls and doc fragments. Additionally, the plain text ratio category metric can be used to prevent our algorithm to falsely classify a fragment that belongs to a certain plain text ratio category. Furthermore, we during this process, while still searching for a light-weight metric that could yield good results we used the longest common subsequence for distinguishing between doc and xls files. Although the precision of this metric proved to be in pair with the results Calhoun presented [], the speed of this approach is way to slow to be used in real life situations.

\section{Individual Null Byte Frequency}
We applied several statistical metrics such as median, mean, mode, standard deviation, minimum and maximum frequency byte values in the BFAs output fragments. However, we couldn't find something that could aid our algorithms design. Then we manually inspected several fragments from all the file types, and we noticed that the amount of null bytes in xls fragments was significantly high. However, although slightly less, the frequencies of null bytes was similar for doc and pdf fragments. We noticed that there were many long sequences of null bytes in most of the pdf and doc but in the xls fragments these sequences were significantly fewer. Additionally, the majority of the total null bytes in xls fragments were individual. Therefore, we analysed the distribution of individual null bytes for all the document-type fragments. As you can see in the figure 1,2,3,4 the frequency of individual null bytes in xls fragments is quite high. For text fragments is 0 and for pdf and doc fragments the frequency mainly ranges from 0 to 25. We should not that for all the 186,345 text fragments that were correctly classified as text from the BFA, both the maximum and minimum null byte and individual null byte frequencies were 0. This is completely reasonable, since text files contain mainly bytes that were inserted from a keyboard and null bytes cannot be typed in an editor. At least not without using some form of hackery.*** TO CHECK


\section{Plain Text Concentration Categories}
As we already mentioned, file fragments of certain types are expected to have certain plain text concetration. We use 4 concentration categories of equal size. 0-25\%, 25-50\%, 50-75\% and 75-100\%. Our metric assumes that fragments are of 512-bytes size. As we saw in Table **4.2**, 75\% and more of text fragments is plain text, the majority of xls fragments(97\%) are 0-50\% plain text. Moreover more than 90\% of the total mp4, zip, ppt, jpg, png and ogg fragments are 25-50\% plain text. We are positive that this light weight metric can be combined with current techniques and increase their accuracy. For example if a fragment is classified as text and less than 75\% of it is plain text, then probably it's not a text fragment. So a classification algorithm could make this simple check and substitute its first classification "guess" with the one that had the second highest accuracy level. Similarly, if a fragment is more than 75\% plain text then probably it's not a mp4, zip or ogg fragment etc.

\section{Shannon Entropy}
There is a widespread use of the Shannon entropy[Shannon] metric in file carving techniques. Entropy measures how much information a sequence of symbols contain[Shannon,Calhoun]. Entropy is defined as:
 \begin{displaymath}
 H({X_i}..{X_n})=-\sum_{i=0}^{n}{p({x_i})}\log_2{p({x_i})}
\end{displaymath} 

In our case, $X={X_i}..{X_n}$  is the byte-content of a fragment, where $n=511$  and $p({x_i})$ is the frequency of ${x_i}$ in ${X}$. To calculate $p({x_i})$, we simply divide the number of occurrences of ${x_i}$ in a fragment with the fragments size. It is known that usually compressed files have high entropy in contrast with text files that have low entropy[Calhoun,Jeroen-Thesis]. Since pdf is a compressed file format, we expect that pdf fragments will have significantly higher entropy than doc, xls and text fragments. In figures 1,2,3,4 we can see the entropy distribution among these file fragments. Most of the pdf fragments have an entropy value of 6 or more, in contrast with the other file-type fragments where the majority of entropy is below 6. In this project, we use this metric to distinguish pdf from xls, doc and text fragments.

\section{Longest Common Subsequence}
While trying to find a way to reduce false positives of the doc and xls fragment classification, we thought to test the performance and accuracy of the longest common subsequence technique. Calhoun[] used this technique to distinguish between fragments of two different file types. He achieved high accuracy results using the standard dynamic programming version of the algorithm, although his small testing set(50 fragments per file type). Even though the dynamic version is faster than the naive approach of the algorithm, with runtime complexity $mxn$, where $m n $ the length of the strings, it still looks as a heavy technique to be used in file carving. He extracted the longest common subsequences of every file fragment in his training set and concatenated them in a big string. This string is representative of the respective file type.  Due to the fact that the speed of this technique depends on the length of the input strings, it is essential to know how long the file type representative string should be. Since he does not provide information about the length of the strings that he used as file type representatives , we want to find out strings of what length can be used as file type representatives and achieve similar results. If the lengths are not too long then the computation of the longest common subsequence between two strings will be faster. Instead of concatenating every longest common subsequence between fragments of the same file type, we used a different approach. We used 500 fragments of doc and xls types for our representative string creation. This resulted to $500x500 - 500 = 249500$ for each file type. We gathered all longest common subsequences from these comparisons and we putted them in a map data structure. Then we sorted the map and we took the first 100, 500, 1000 and 1500 most frequent. Then we concatenated these subsequences in 4 big strings. We ended up with 4 pretty long representative strings for each of the doc and xls file type.
 