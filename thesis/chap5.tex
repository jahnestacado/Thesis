\chapter{Classification  Metrics}
\section{BFA Variation 1 Output}
After the run of variation 1 BFA, we isolated all fragments which were classified as text. We believed that BFA falsely classifies fragments from non-text files as text, due to their high plain text concentration. We conducted a plain text concentration analysis on the BFAs output and it seems that BFA classified as text fragments with both very low and high plain text concentration. This analysis can be found in Table X.

 Since the 85\% of them originates from xls, pdf, doc and text files, we consider the remaining 15\%, which corresponds to fragments from the other 6 file types, that do not exist. So by doing this, we expect that the amount of fragments that were falsely classified as text without belonging to a document-type file, will be evenly distributed among the false positive classification results for xls, pdf, doc and text fragments. Our algorithms goal is to be able to correctly identify and distinguish between xls, pdf, doc and text fragments. For that purpose we conducted statistical analysis in the BFAs output trying to find patterns that will help as increase our algorithms accuracy. We introduce two new metrics, the individual null byte frequency and the plain text ratio category.  The individual null byte frequency in conjunction with shannon entropy[] can be used to effectively distinguish  between pdf from xls and doc fragments. Additionally, the plain text ratio category metric can be used to prevent our algorithm to falsely classify a fragment that belongs to a certain plain text ratio category. Furthermore, we during this process, while still searching for a light-weight metric that could yield good results we used the longest common subsequence for distinguishing between doc and xls files. Although the precision of this metric proved to be in pair with the results Calhoun presented [], the speed of this approach is way to slow to be used in real life situations.

\section{Individual Null Byte Frequency}
We applied several statistical metrics such as median, mean, mode, standard deviation, minimum and maximum frequency byte values in the BFAs output fragments. However, we couldn't find something that could aid our algorithms design. Then we manually inspected several fragments from all the file types, and we noticed that the amount of null bytes in xls fragments was significantly high. However, although slightly less, the frequencies of null bytes was similar for doc and pdf fragments. We noticed that there were many long sequences of null bytes in most of the pdf and doc but in the xls fragments these sequences were significantly fewer. Additionally, the majority of the total null bytes in xls fragments were individual. Therefore, we analysed the distribution of individual null bytes for all the document-type fragments. As you can see in the figure 1,2,3,4 the frequency of individual null bytes in xls fragments is quite high. For text fragments is 0 and for pdf and doc fragments the frequency mainly ranges from 0 to 25. We should not that for all the 186,345 text fragments that were correctly classified as text from the BFA, both the maximum and minimum null byte and individual null byte frequencies were 0. This is completely reasonable, since text files contain mainly bytes that were inserted from a keyboard and null bytes cannot be typed in an editor. At least not without using some form of hackery.*** TO CHECK


\section{Plain Text Concentration Categories}
As we already mentioned, file fragments of certain types are expected to have certain plain text concetration. We use 4 concentration categories of equal size. 0-25\%, 25-50\%, 50-75\% and 75-100\%. Our metric assumes that fragments are of 512-bytes size. As we saw in Table **4.2**, 75\% and more of text fragments is plain text, the majority of xls fragments(97\%) are 0-50\% plain text. Moreover more than 90\% of the total mp4, zip, ppt, jpg, png and ogg fragments are 25-50\% plain text. We are positive that this light weight metric can be combined with current techniques and increase their accuracy. For example if a fragment is classified as text and less than 75\% of it is plain text, then probably it's not a text fragment. So a classification algorithm could make this simple check and substitute its first classification "guess" with the one that had the second highest accuracy level. Similarly, if a fragment is more than 75\% plain text then probably it's not a mp4, zip or ogg fragment etc.

\section{Shannon Entropy}

 \begin{displaymath}
 H(X)=-\sum_{i=0}^{n}{p_i}\log_2{p_i}
\end{displaymath} 
