\chapter{Analysis}
In this chapter we will analyse the results of chapter \ref{chap:6}. We divide this chapter in three sections. In section \ref{sec:7.1} we analyse the performance of BFA and the amount of fragments that were classified as text. In section  \ref{sec:7.2} we analyse the performance of our complete algorithm. Finally in section  \ref{sec:7.3} we compare our algorithm performance with the classification algorithms that Shahi tested in \cite{Shahi}.

\section{BFA performance}\label{sec:7.1}
In our final test, the BFA part of our algorithm performed as expected. It yielded almost identical results as the results that we got during the algorithms development procedure (chapter \ref{chap:6}).

In a corpus of 18.714.081 fragments BFA classified 5.844.621 as text. From that amount 4.981.786 of fragments do originate from document-type fragments and 862.835 from the other 6 file types. This means that we have a percentage of 85,2\% true positives and a 14,8\% of false positives regarding text fragment classification. These results are in pair with the ones that we got when we used this variation of BFA in a ten-times smaller corpus(Table \ref{table:table 4.1}).

Moreover, this 85,2\% of true positives correspond to the 67,2\% of the total doc, pdf, xls and text fragments of our initial corpus. However, this percentage is not representative for a real life case since our final testing set was comprised only of fragments that had at least one plain text character. Considering that for each one of the 10 file types that we use we know the percentage of fragments that do not contain plain text(**TODO**Table in chapter 3), we can calculate the approximate overall document-type fragment retrieval in a corpus of 10GB size. Thus, taking under account the percentages of (**TODO**Table in chapter 3) that correspond to fragments with 0\% plain text concentration, the BFA would have approximately retrieved the *TODO**\% of the total doc, pdf, xls and text fragments of a 10GB corpus.

\section{Overall Algorithm Performance}\label{sec:7.2}
The accuracies for the 4 file types of our interest are quite encouraging. We got 9,5\% for pdf, 98,8\% for text, 32,5\% for doc, 71,3\% for xls fragments and 92,4\% for classifying a fragments as of another file type. It seems that due to the high entropy of the non-document type fragments, most of the fragments that were initially falsely classified as text, thereafter were falsely classified as pdf. This is not necessarily a bad thing, since it kept in low levels the false positives rates for the doc, xls and text file types. Especially for the text and xls file types the false positive rates are extremely small.

However, the information that we can get from this results are insufficient to evaluate our algorithms performance. In addition, this is the main problem with the scientific publications in the file carving field. We observed that most of the papers upon file carving techniques provide only the true positive rates and doesn't give more information regarding the classification capacity of their algorithms. Surprisingly enough, this is in general a common phenomenon in algorithm comparison studies \cite{Demsar}\cite{Sokolova}.

We consider that by calculating the $F-score$ along with the overall accuracy of our algorithm we can acquire better insight for our algorithms performance. The $F-score$  is a statistical measure that tests accuracy and it considers both the precision and recall  measures of the test to compute the score. A higher $F-score$ implies higher accuracy. The $F-score$ measurement is described in formula ~\ref{formula:fscore}. Note that $tp$ stands for "true positive", $fp$ for "false positive", $tn$ for "true negative" and $fn$ for "false negative".




\noindent\begin{minipage}{.5\linewidth}
\small

\begin{equation}
precision = \frac{tp}{tp + fp}
\end{equation}
\end{minipage}%
\begin{minipage}{.5\linewidth}
\small
\begin{equation}
recall = \frac{tp}{tp + fn}
\end{equation}
\end{minipage}%

\begin{center}
\begin{minipage}{.5\linewidth}
\small
\begin{equation}
\newline
F-score = 2 * \frac{precision * recall}{precision + recall}
\end{equation}
\label{formula:fscore}
\end{minipage}
\end{center}

Additionally, the equation that we use to calculate overall accuracy can be found in formula \ref{overall_accuracy}. *** FIx link numbers***

\begin{center}
\noindent\begin{minipage}{.5\linewidth}
\small
\begin{equation}
overall \ accuracy = \frac{correct\ predictions}{total\ predictions}  
\end{equation}
\label{overall_accuracy}
\end{minipage}
\end{center}

The results of the aforementioned measurements can be found in Table ~\ref{table:overall_results}. Our classifier performed extremely well for the text and xls file types with an ${F_1}-score$ of 0,91 and 0,78 respectively. The prediction rates  for the doc file type are quite satisfying with an ${F_1}-score$  of 0,39. We should note that the doc file type was the most "tricky" among the document-file types that we analysed, due to the fact that we couldn't find strong distinctive characteristics. Furthermore, the worst rates are for the pdf file type, mainly because BFA classified as text only the 20\% of the initial pdf fragments. Additionally, during the second phase of our classification algorithm most of non-document type fragments were falsely classified as pdf due to their high entropy. Finally, the overall accuracy of our classifier is 0,77, a very high number for a file carving technique. This means that if our classifier does 100 predictions, 77 of them will be correct.
\input{overall_results_table}

\section{Algorithm Comparison}
In this section we try to compare the the performance of our classifier with similar classification techniques. In subsection ~\ref{sub:accuracy} we compare the accuracies of the algorithms and in subsection ~\ref{sub:speed} we compare their performance in terms of execution time.

Among the algorithms that we compare are the Byte Frequency Analysis algorithm\cite{MacDaniel}, the n-Gram Analysis\cite{ngram}, the Rate of Change\cite{roc} and the algorithm of Conti et al.\cite{Conti}. We use the results that Shahi acquired from his experiment\cite{Ashim}. We chose to compare our algorithms performance with Shahis results, because he used the same file types for his experiment and his testing set was of the exact same size as our final testing set. One of the major problems in the file carving field is that there is no proper comparison between classification algorithms, since every technique was tested in different data sets that were comprised of different file types. Furthermore, at this point we should note that for the algorithms comparison, we use the results that Shahi got by using fingerprints that were trained with the 10\% of his total training set. This 10\% corresponds to the size of the training set that we used to train our fingerprints.


\subsection{Accuracy Comparison}\label{sub:accuracy}
Although the testing sets that were used for all the experiments are similar, it is still quite difficult to compare these algorithms. Our algorithm was specifically designed to classify only fragments of 4 file types or to classify as "other" everything that was classified as non-text from its BFA part. Moreover, all algorithms do not give an $F-score$ for every file type. The results can be found in Table ~\ref{comparison_table}.

\input{alg_comparison_table} 

Our algorithm outperforms the other 4 in classifying fragments of the text file format with an $F-score$ of 0,91. Furthermore, it comes second only after the Rate of Change algorithm regarding xls and doc fragment classification. Lastly, the $F-score$ for the pdf file types is the worst along with the algorithm of Conti et al. score. Moreover, our algorithms overall accuracy is approximately 2,5 times higher than the overall accuracies of the other algorithms. However, we should not forget that our algorithm is limited in classifying only fragments that contain at least one plain text character. Overall, although our algorithm takes less decisions and classifies non-document fragments in a broad class, its decisions are significantly more reliable than the decisions of its competitors.

\subsection{Execution Time Comparison}\label{sub:speed}



