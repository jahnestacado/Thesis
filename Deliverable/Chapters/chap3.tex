\chapter{Experimental Setup}

\section{Byte Frequency Analysis(BFA) Algorithm}
BFA\cite{MacDaniel} is a statistical learning algorithm that was initially developed to analyse and classify whole files. It was not meant to be used for file fragment classification. By counting the number of instances of each byte in a file of a certain type, BFA creates a representative average value for each byte instance, among with its respective correlation strength. This results in a fingerprint of a particular file-type. Thereafter, during the classification procedure, the input file is compared with every fingerprint and an accuracy level is created for each of them. BFA classifies the file to the file type of the fingerpint that corresponds to the highest accuracy level.

Shahi\cite{Ashim} trained and tested the BFA with file fragments of 512-byte size. His results show that although the algorithm is pretty bad for broad fragment classification, it is quite good in classifying fragments that belong to document-type files, as text. He tested the performance of BFA, along with the Byte Frequency Correlation algorithm, n-Gram Analysis and Conti et al. algorithm. The results show that BFA has the highest precision in classifying document-type fragments as text.

In contrast to the default technique, we use a BFA that trains our fingerprints with byte values that correspond only to the plain text ASCII characters, instead of the complete byte-set of the fragments. We also use fragments of 512-bytes size. This BFA will be the first half of our final algorithm and after this point we intend to use additional metrics to create a custom classifier. Taking under account speed requirements, BFA seems as a good candidate since it is a lightweight technique, compared to similar statistical learning algorithms\cite{Ashim} or heavier machine learning techniques. 

\section{Data Set}
The data set we used for our training, experimentation, analysis and testing procedures is derived from  Garfinkels\cite{Garfinkel}\cite{Corpora} coprus, Wikipedia and Academic Earth~\cite{A.Earth} and is a subset of the coprus that Shahi used in \cite{Ashim}. Our corpus is comprised of 10 different file types with a total size of about 20GB. We divided the corpus in half, resulting in two subsets of 10GB each. The experimental and the final testing set.

We use the experimental set to do all of our experimentations, analysis and training,  and the final testing set for testing the performance of our final algorithm.
At this point we should note that the 10GB that corresponds to the final testing set wont undergone any type of analysis that will affect the design of our algorithm, since we only want to use it for testing our final algorithm. We fully designed our algorithm based only on the experimental set.
\input{./Tables/data_set_table}
In the experimental set, we split these 10GB in two subsets of 9-1 ratio.  90\% of the experimental set is used as our training set and the other 10\% as our experimental testing set. Additionally, we transformed all of our files content, in both the experimentation and the final testing set, into 512-byte blocks, which we refer to them as fragments. Since our algorithm would be able to classify only fragments that contain at least one plain text character, fragments with no plain text were discarded. The percentage of discarded fragments per file type can be found in Table ~\ref{table:data set}. As we can see, the percentage of fragments with no plain text for most of the file types is around 5\%. Surprisingly enough, this percentage is significantly higher(10-17\%) for the doc file type. This is quite interesting since files of the doc type are documents, which are mainly comprised of plain text characters.

Furthermore, we used our training set to train our fingerprints and the experimental testing set to test all 4 variations of our BFA algorithm. Both of the aforementioned sets undergone statistical analysis in order to discover useful patterns. More detailed information about our experimental data set can be found in Table ~\ref{table:data set}.


Information regarding the final testing set that we used to test the performance of our final algorithm can be found in Table \ref{table:final set}.
\input{./Tables/final_testing_dataset}








